# -*- coding: utf-8 -*-
"""Question_C.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14EEcDp6oIpsZJkd38CQjY0YwmvQ8PIrD

c. train, validate and test your ANN on regression data of a Combined Cycle Power Plant Dataset

1. Data Splitting
"""

import pandas as pd
import numpy as np


# Load the dataset
file_path = "Folds5x2_pp.xlsx"  # Replace 'your_dataset.xlsx' with the actual file path
data = pd.read_excel(file_path)

# Display the first few rows of the dataset
print(data.head())


# Extract features and labels
features = data.iloc[:, :-1].values  # Assuming last column is the target variable
labels = data.iloc[:, -1].values

# Determine the number of samples
num_samples = len(features)

# Calculate the number of samples for each set
num_train = int(num_samples * 0.72)
num_val = int(num_samples * 0.18)
num_test = num_samples - num_train - num_val

# Shuffle the indices
indices = np.random.permutation(num_samples)

# Split the indices into training, validation, and test sets
train_indices = indices[:num_train]
val_indices = indices[num_train:num_train + num_val]
test_indices = indices[num_train + num_val:]

# Split the data into training, validation, and test sets
x_train, y_train = features[train_indices], labels[train_indices]
x_val, y_val = features[val_indices], labels[val_indices]
x_test, y_test = features[test_indices], labels[test_indices]

# Ensure validation and test sets are continuous chunks
# Assuming test set is taken from the end of the dataset
x_test, y_test = features[-num_test:], labels[-num_test:]
x_val, y_val = features[-num_val - num_test:-num_test], labels[-num_val - num_test:-num_test]

# Print the shapes of the datasets
print("Training set shape:", x_train.shape, y_train.shape)
print("Validation set shape:", x_val.shape, y_val.shape)
print("Test set shape:", x_test.shape, y_test.shape)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Define the tanh activation function and its derivative
def tanh(Z):
    return np.tanh(Z)

def tanh_derivative(Z):
    return 1 - np.tanh(Z)**2

# Initialize parameters for the neural network
def initialize_parameters(input_dim, hidden_dims, output_dim):
    parameters = {}
    layer_dims = [input_dim] + hidden_dims + [output_dim]

    for l in range(1, len(layer_dims)):
        parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])
        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))

    return parameters


# Forward propagation through the network
def forward_propagation(X, parameters):
    caches = []
    A = X
    L = len(parameters) // 2

    for l in range(1, L):
        A_prev = A
        Z = np.dot(parameters[f'W{l}'], A_prev) + parameters[f'b{l}']
        A = tanh(Z)
        cache = (A_prev, Z)
        caches.append(cache)

    AL = np.dot(parameters[f'W{L}'], A) + parameters[f'b{L}']

    return AL, caches
def backward_propagation(X, Y, parameters, caches):
    grads = {}
    L = len(caches)
    m = X.shape[1]
    AL, cache = caches[-1]
    dAL = (1./m) * (AL - Y)

    dZL = dAL * tanh_derivative(cache[1])
    grads[f'dW{L}'] = np.dot(dZL, cache[0].T)
    grads[f'db{L}'] = np.sum(dZL, axis=1, keepdims=True)
    grads[f'dA{L-1}'] = np.dot(parameters[f'W{L}'].T, dZL)

    for l in reversed(range(L)):  # Adjusted loop range
        A_prev, Z = caches[l]
        if l == L - 1:
            dA = dAL
        else:
            dA = grads[f'dA{l+1}']
        dZ = dA * tanh_derivative(Z)
        grads[f'dW{l+1}'] = np.dot(dZ, A_prev.T)
        grads[f'db{l+1}'] = np.sum(dZ, axis=1, keepdims=True)
        if l > 0:
            grads[f'dA{l}'] = np.dot(parameters[f'W{l+1}'].T, dZ)  # Update dA for hidden layers

    return grads



def update_parameters(parameters, grads, learning_rate):
    for key in parameters.keys():
        if 'W' in key and 'd' + key in grads:
            parameters[key] -= learning_rate * grads['d' + key]
    return parameters



# Train the neural network
def train_neural_network(X_train, Y_train, hidden_dims, num_epochs, learning_rate):
    input_dim = X_train.shape[0]
    output_dim = Y_train.shape[0]
    parameters = initialize_parameters(input_dim, hidden_dims, output_dim)
    costs = []

    for epoch in range(num_epochs):
        AL, caches = forward_propagation(X_train, parameters)
        cost = np.mean((AL - Y_train) ** 2)
        costs.append(cost)
        grads = backward_propagation(X_train, Y_train, parameters, caches)
        parameters = update_parameters(parameters, grads, learning_rate)

        if epoch % 100 == 0:
            print(f"Epoch {epoch}/{num_epochs}, Cost: {cost}")

    return parameters, costs

# Load and preprocess the dataset
def load_dataset(filepath):
    data = pd.read_excel(filepath)
    X = data.iloc[:, :-1].values.T
    Y = data.iloc[:, -1].values.reshape(1, -1)
    return X, Y

# Normalize the input features
def normalize(data):
    mean = np.mean(data, axis=1, keepdims=True)
    std = np.std(data, axis=1, keepdims=True)
    normalized_data = (data - mean) / std
    return normalized_data, mean, std

# Denormalize function
def denormalize(data, mean, std):
    return data * std + mean

# Calculate Mean Absolute Percentage Error (MAPE)
def calculate_mape(Y_true, Y_pred):
    epsilon = 1e-10
    mape = np.mean(np.abs((Y_true - Y_pred) / (Y_true + epsilon))) * 100
    return mape

# Main code
filepath = "/content/Folds5x2_pp.xlsx"

X, Y = load_dataset(filepath)

# Normalize the input features
X_normalized, X_mean, X_std = normalize(X)
Y_normalized, Y_mean, Y_std = normalize(Y)

# Split data into training and testing sets
train_ratio = 0.8
train_samples = int(train_ratio * X.shape[1])

X_train = X_normalized[:, :train_samples]
Y_train = Y_normalized[:, :train_samples]
X_test = X_normalized[:, train_samples:]
Y_test = Y_normalized[:, train_samples:]

# Define hyperparameters
hidden_dims = [10, 10]  # Adjusted definition
num_epochs = 1000
learning_rate = 0.01

# Train the neural network
parameters, costs = train_neural_network(X_train, Y_train, hidden_dims, num_epochs, learning_rate)


# Plot the cost over epochs
plt.plot(range(num_epochs), costs)
plt.xlabel('Epoch')
plt.ylabel('Cost')
plt.title('Training Cost')
plt.show()

# Test the neural network
def test_neural_network(X_test, Y_test, parameters):
    AL, _ = forward_propagation(X_test, parameters)
    cost = np.mean((AL - Y_test) ** 2)
    return cost

# Evaluate the neural network
test_cost = test_neural_network(X_test, Y_test, parameters)
print(f"Test Cost: {test_cost}")

# Denormalize the predictions and true values
Y_pred_normalized, _ = forward_propagation(X_test, parameters)
Y_pred = denormalize(Y_pred_normalized, Y_mean, Y_std)
Y_true = denormalize(Y_test, Y_mean, Y_std)

# Calculate MAPE
mape = calculate_mape(Y_true, Y_pred)
print(f"Mean Absolute Percentage Error (MAPE): {mape}%")

"""The provided code implements a neural network for regression using numpy. The network architecture consists of multiple hidden layers with the tanh activation function. The dataset used for training and testing the model is loaded from an Excel file.

Results:

Training Cost Plot: The plot shows the decrease in training cost (mean squared error) over the epochs.

Test Cost: The mean squared error on the testing set is calculated to evaluate the generalization performance of the model. In this case, the final test cost is 0.7209.

MAPE: The mean absolute percentage error indicates the average deviation of the model's predictions from the actual values. In this case, MAPE is 2.67%.
"""

