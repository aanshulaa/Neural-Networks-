# -*- coding: utf-8 -*-
"""Question_a&bipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sBdJzeYzFcwI_tUVNUZ8mac7LVb1DQhq

Building an ANN using Backpropagation and experimenting with multiple tools

a. Listing out the step-by-step backpropagation algorithm, with the equations and control actions of the program

Step 1: Initialization

Initialize the weights and biases randomly.
Define the learning rate and the number of epochs.

Step 2: Forward Pass

Perform a forward pass through the network to compute the predicted output.

Step 3: Compute Loss

Calculate the loss using a suitable loss function (e.g., mean squared error).

Step 4: Backward Pass

Perform a backward pass to compute gradients of the loss with respect to weights and biases.

Step 5: Update Weights and Biases

Update the weights and biases using the gradients computed in the backward pass.

Step 6: Repeat

Repeat steps 2-5 for a fixed number of epochs or until convergence.
"""

import numpy as np

# Define the activation function (sigmoid)
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Define the derivative of the activation function
def sigmoid_derivative(x):
    return x * (1 - x)

# Define the neural network class
class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        # Initialize weights and biases randomly
        self.weights_input_hidden = np.random.randn(input_size, hidden_size)
        self.bias_input_hidden = np.random.randn(1, hidden_size)
        self.weights_hidden_output = np.random.randn(hidden_size, output_size)
        self.bias_hidden_output = np.random.randn(1, output_size)

    def forward(self, inputs):
        # Perform forward pass
        self.hidden_layer_input = np.dot(inputs, self.weights_input_hidden) + self.bias_input_hidden
        self.hidden_layer_output = sigmoid(self.hidden_layer_input)
        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_hidden_output
        self.output = sigmoid(self.output_layer_input)
        return self.output

    def backward(self, inputs, targets, learning_rate):
        # Compute gradients using backpropagation
        error = targets - self.output
        delta_output = error * sigmoid_derivative(self.output)
        error_hidden = np.dot(delta_output, self.weights_hidden_output.T)
        delta_hidden = error_hidden * sigmoid_derivative(self.hidden_layer_output)

        # Update weights and biases
        self.weights_hidden_output += np.dot(self.hidden_layer_output.T, delta_output) * learning_rate
        self.bias_hidden_output += np.sum(delta_output, axis=0, keepdims=True) * learning_rate
        self.weights_input_hidden += np.dot(inputs.T, delta_hidden) * learning_rate
        self.bias_input_hidden += np.sum(delta_hidden, axis=0, keepdims=True) * learning_rate

    def train(self, inputs, targets, learning_rate, epochs):
        for epoch in range(epochs):
            output = self.forward(inputs)
            self.backward(inputs, targets, learning_rate)
            loss = np.mean(np.square(targets - output))
            if epoch % 1000 == 0:
                print(f'Epoch {epoch}: Loss = {loss}')

# Example usage
if __name__ == "__main__":
    # Define dataset
    inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    targets = np.array([[0], [1], [1], [0]])

    # Define neural network
    input_size = 2
    hidden_size = 2
    output_size = 1
    learning_rate = 0.1
    epochs = 10000
    nn = NeuralNetwork(input_size, hidden_size, output_size)

    # Train the neural network
    nn.train(inputs, targets, learning_rate, epochs)

    # Test the trained neural network
    print("\nFinal predictions:")
    for i in range(len(inputs)):
        print(f"Input: {inputs[i]} | Prediction: {nn.forward(inputs[i])}")

"""This code implements a basic neural network with one hidden layer

Training and validating the ANN on the toy problem of learning the mathematical function y = sin(x),
where −2π ≤ x ≤ 2π
"""

import numpy as np
import matplotlib.pyplot as plt

# Step b1: Training Data Preparation
num_points_per_part = 250
x_train = np.linspace(-2 * np.pi, 2 * np.pi, num_points_per_part * 4)
y_train = np.sin(x_train)

# Step b2: Validation Data Preparation
num_validation_points = 300
x_validation = np.random.uniform(-2 * np.pi, 2 * np.pi, num_validation_points)

# Step b3: Plotting
plt.figure(figsize=(8, 6))
plt.plot(x_train, y_train, label='Training Data (sin(x))', color='blue')
plt.scatter(x_validation, np.sin(x_validation), label='Validation Data (sin(x))', color='red', marker='x')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Training and Validation Data')
plt.legend()
plt.grid(True)
plt.show()

"""b1. We prepare the training data by extracting 1000 (x, y) pairs equally distributed within the domain
−2π≤x≤2π. We'll split the total domain 4π into four equal parts and extract 250 points from each part with equal intervals.
"""

import numpy as np

# Define the sigmoid activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Define the neural network class
class NeuralNetwork:
    def __init__(self, input_size, output_size):
        self.input_size = input_size
        self.output_size = output_size

        # Initialize weights and biases
        self.weights = np.random.randn(self.input_size, self.output_size)
        self.biases = np.zeros((1, self.output_size))

    def forward(self, x):
        # Forward pass through the network
        self.input = x
        self.output = sigmoid(np.dot(self.input, self.weights) + self.biases)
        return self.output

    def backward(self, x, y, output, learning_rate):
        # Backpropagation
        error = y - output
        delta = error * sigmoid_derivative(output)

        # Update weights and biases
        self.weights += np.dot(x.T, delta) * learning_rate
        self.biases += np.sum(delta, axis=0, keepdims=True) * learning_rate

    def train(self, x_train, y_train, learning_rate, num_epochs):
        for epoch in range(num_epochs):
            # Forward pass
            output = self.forward(x_train)

            # Backward pass
            self.backward(x_train, y_train, output, learning_rate)

            # Compute and print loss (MSE)
            loss = np.mean((y_train - output) ** 2)
            if epoch % 100 == 0:
                print(f'Epoch {epoch}, Loss: {loss:.4f}')

# Define the training parameters
input_size = 1
output_size = 1
learning_rate = 0.01
num_epochs = 1000

# Generate 1000 (x, y) pairs equally distributed within the domain -2pi <= x <= 2pi
num_points_per_part = 250
x_train = np.linspace(-2 * np.pi, 2 * np.pi, num_points_per_part * 4)
y_train = np.sin(x_train)

# Reshape x_train and y_train to column vectors
x_train = x_train.reshape(-1, 1)
y_train = y_train.reshape(-1, 1)

# Create and train the neural network
model = NeuralNetwork(input_size, output_size)
model.train(x_train, y_train, learning_rate, num_epochs)

import numpy as np

# Number of points to extract from each part
num_points_per_part = 250

# Define the domain
x_min = -2 * np.pi
x_max = 2 * np.pi

# Split the domain into four parts
x_parts = np.linspace(x_min, x_max, 5)

# Remove the last element since linspace includes both endpoints
x_parts = x_parts[:-1]

# Initialize lists to store x and y values
x_values = []
y_values = []

# Generate 250 points from each part
for i in range(len(x_parts) - 1):
    x_part = np.linspace(x_parts[i], x_parts[i + 1], num_points_per_part, endpoint=False)
    y_part = np.sin(x_part)  # Compute y values using y = sin(x)
    x_values.extend(x_part)
    y_values.extend(y_part)

# Convert lists to NumPy arrays
x_train = np.array(x_values)
y_train = np.array(y_values)

# Reshape x_train to a column vector if needed (depends on your ANN implementation)
x_train = x_train.reshape(-1, 1)

# Print the generated (x, y) pairs
print("Generated (x, y) pairs for training data:")
for x, y in zip(x_train, y_train):
    print(f"x: {x}, y: {y}")

"""This code generates 1000 (x, y) pairs equally distributed within the domain −2π≤x≤2π.We use these pairs to train the neural network, which has one input neuron and one output neuron

b2.Generating 300 points randomly within the range −2π≤x≤2π using a uniform distribution random number generator in Python (NumPy). These x-values will be used as validation data, and the ANN will generate the corresponding y-values as outputs.
"""

import numpy as np

# Define the number of points for validation data
num_validation_points = 300

# Generate 300 points randomly within the range -2π≤x≤2π
x_validation = np.random.uniform(-2 * np.pi, 2 * np.pi, num_validation_points)

# Reshape x_validation to a column vector
x_validation = x_validation.reshape(-1, 1)

# Print the shape of the validation data
print("Shape of validation data (x_validation):", x_validation.shape)

print(x_validation)

"""Now we have generated 300 points randomly within the range −2π≤x≤2π using a uniform distribution random number generator in NumPy. It then prints out the generated x-values for validation data.

b3.Plotting the x-y curves extracted from the data in b1 on the same plot extracted from the outputs of your
ANN in b2.
"""

import numpy as np
import matplotlib.pyplot as plt

# Assuming `model` is your trained ANN

# Generate x values for plotting sine function from training data
x_train_plot = np.linspace(-2 * np.pi, 2 * np.pi, 1000)
y_train_plot = np.sin(x_train_plot)

# Generate y values using the ANN for the validation data
y_ann_validation = model.forward(x_validation).reshape(-1)

# Plot the ground truth sine curve from training data
plt.plot(x_train_plot, y_train_plot, label='Ground Truth (sin(x)) from Training Data', color='blue')

# Plot the outputs generated by the trained ANN using validation data
plt.scatter(x_validation, y_ann_validation, label='ANN Output from Validation Data', color='red', marker='x')

# Add labels and title
plt.xlabel('x')
plt.ylabel('y')
plt.title('Comparison of Ground Truth and ANN Output')
plt.legend()

# Show plot
plt.grid(True)
plt.show()

"""In this code:

We generate x values for plotting the sine function curve (y=sin(x)) from the training data.

We use the validation data x_validation from Step b2 and the corresponding output of the trained ANN to plot the ANN output curve.

We add labels, a title, and a legend to the plot.

We display the plot showing the comparison between the ground truth and the ANN output.

This plot will help you visually assess how well the ANN has learned to approximate the sine function. If the two curves align closely, it indicates that the ANN has learned the function correctly.

b4.

ANN Architecture
"""

import numpy as np

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Initialize weights and biases
        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)
        self.biases_input_hidden = np.zeros((1, self.hidden_size))
        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)
        self.biases_hidden_output = np.zeros((1, self.output_size))

    def tanh(self, x):
        return np.tanh(x)

    def tanh_derivative(self, x):
        return 1 - np.tanh(x) ** 2

    def forward(self, x):
        # Forward pass
        self.hidden_input = np.dot(x, self.weights_input_hidden) + self.biases_input_hidden
        self.hidden_output = self.tanh(self.hidden_input)
        self.output = np.dot(self.hidden_output, self.weights_hidden_output) + self.biases_hidden_output
        return self.output

    def backward(self, x, y, output, learning_rate):
        # Backward pass
        output_error = y - output
        output_delta = output_error * self.tanh_derivative(output)
        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)
        hidden_delta = hidden_error * self.tanh_derivative(self.hidden_output)

        # Update weights and biases
        self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate
        self.biases_hidden_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate
        self.weights_input_hidden += np.dot(x.T, hidden_delta) * learning_rate
        self.biases_input_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate

    def train(self, x_train, y_train, learning_rate, num_epochs):
        for epoch in range(num_epochs):
            # Forward pass
            output = self.forward(x_train)

            # Backward pass
            self.backward(x_train, y_train, output, learning_rate)

            # Compute and print loss (MSE)
            loss = np.mean((y_train - output) ** 2)
            if epoch % 100 == 0:
                print(f'Epoch {epoch}, Loss: {loss:.4f}')

# Define the parameters
input_size = 1  # Assuming one input variable
output_size = 1  # Assuming one output variable
num_training_samples = 1000  # Number of training data samples

# Determine the number of neurons in the hidden layer based on the rule of thumb
max_neurons_rule_of_thumb = num_training_samples // 2
hidden_size = max_neurons_rule_of_thumb  # We'll use the maximum number of neurons allowed

# Define the training data
x_train = np.linspace(-2 * np.pi, 2 * np.pi, num_training_samples).reshape(-1, 1)
y_train = np.sin(x_train)

# Normalize the input data
x_train_normalized = (x_train - np.min(x_train)) / (np.max(x_train) - np.min(x_train))

# Define the hyperparameters
learning_rate = 0.01
num_epochs = 1000

# Create and train the neural network
model = NeuralNetwork(input_size, hidden_size, output_size)
model.train(x_train_normalized, y_train, learning_rate, num_epochs)

"""In this implementation:

We define a neural network with one hidden layer.

The number of neurons in the hidden layer is determined based on the rule of thumb, ensuring that it does not exceed half the number of training data samples.

We train the neural network using the provided training data (extracted 1000 pairs) and evaluate its performance.

Back-propagation equations (Vectorization)

let's implement the fully vectorized approach where we use matrix operations for both the forward and backward passes without any explicit loops over the samples.
"""

class NeuralNetworkPartiallyVectorized:
    # Implementation of NeuralNetworkPartiallyVectorized class

    def train(self, x_train, y_train, learning_rate, num_epochs):
        start_time = time.time()
        for epoch in range(num_epochs):
            # Iterate over samples for the backward pass
            for x, y in zip(x_train, y_train):
                # Forward pass (unchanged)
                output = self.forward(x.reshape(1, -1))

                # Backward pass (unchanged)
                self.backward(x.reshape(1, -1), y.reshape(1, -1), output, learning_rate)

            # Compute loss and print every 100 epochs (unchanged)
            loss = np.mean((y_train - self.forward(x_train)) ** 2)
            if epoch % 100 == 0:
                print(f'Epoch {epoch}, Loss: {loss:.4f}')
        end_time = time.time()
        training_time = end_time - start_time

        return training_time

import numpy as np
import time

class NeuralNetworkPartiallyVectorized:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Initialize weights and biases
        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)
        self.biases_input_hidden = np.zeros((1, self.hidden_size))
        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)
        self.biases_hidden_output = np.zeros((1, self.output_size))

    def tanh(self, x):
        return np.tanh(x)

    def tanh_derivative(self, x):
        return 1 - np.tanh(x) ** 2

    def forward(self, x):
        # Fully vectorized forward pass
        self.hidden_input = np.dot(x, self.weights_input_hidden) + self.biases_input_hidden
        self.hidden_output = self.tanh(self.hidden_input)
        self.output = np.dot(self.hidden_output, self.weights_hidden_output) + self.biases_hidden_output
        return self.output

    def backward(self, x, y, output, learning_rate):
        # Partially vectorized backward pass with loop over samples
        for xi, yi in zip(x, y):
            # Forward pass for the current sample
            hidden_input = np.dot(xi, self.weights_input_hidden) + self.biases_input_hidden
            hidden_output = self.tanh(hidden_input)
            output_current = np.dot(hidden_output, self.weights_hidden_output) + self.biases_hidden_output

            # Backward pass for the current sample
            output_error = yi - output_current
            output_delta = output_error * self.tanh_derivative(output_current)
            hidden_error = np.dot(output_delta, self.weights_hidden_output.T)
            hidden_delta = hidden_error * self.tanh_derivative(hidden_output)

            # Update weights and biases using vectorized operations
            self.weights_hidden_output += np.dot(hidden_output.T, output_delta) * learning_rate
            self.biases_hidden_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate
            self.weights_input_hidden += np.dot(xi.T, hidden_delta) * learning_rate
            self.biases_input_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate

    def train(self, x_train, y_train, learning_rate, num_epochs):
        start_time = time.time()
        for epoch in range(num_epochs):
            # Fully vectorized forward pass (unchanged)
            output = self.forward(x_train)

            # Partially vectorized backward pass with loop over samples (unchanged)
            self.backward(x_train, y_train, output, learning_rate)

            # Compute loss and print every 100 epochs (unchanged)
            loss = np.mean((y_train - output) ** 2)
            if epoch % 100 == 0:
                print(f'Epoch {epoch}, Loss: {loss:.4f}')
        end_time = time.time()
        training_time = end_time - start_time

        return training_time

# Define the validation data
x_val = np.linspace(-2 * np.pi, 2 * np.pi, 300).reshape(-1, 1)
y_val = np.sin(x_val)

# Normalize the validation data
x_val_normalized = (x_val - np.min(x_train)) / (np.max(x_train) - np.min(x_train))

# Create and train the neural network (partially vectorized)
model_partially_vectorized = NeuralNetworkPartiallyVectorized(input_size, hidden_size, output_size)
training_time_partially_vectorized = model_partially_vectorized.train(x_train_normalized, y_train, learning_rate, num_epochs)

# Evaluate accuracy on validation data
predictions_val_partially_vectorized = model_partially_vectorized.forward(x_val_normalized)
accuracy_partially_vectorized = np.mean(np.abs(predictions_val_partially_vectorized - y_val) < 0.1)

class NeuralNetworkFullyVectorized:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Initialize weights and biases
        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)
        self.biases_input_hidden = np.zeros((1, self.hidden_size))
        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)
        self.biases_hidden_output = np.zeros((1, self.output_size))

    def tanh(self, x):
        return np.tanh(x)

    def tanh_derivative(self, x):
        return 1 - np.tanh(x) ** 2

    def forward(self, x):
        # Fully vectorized forward pass
        self.hidden_input = np.dot(x, self.weights_input_hidden) + self.biases_input_hidden
        self.hidden_output = self.tanh(self.hidden_input)
        self.output = np.dot(self.hidden_output, self.weights_hidden_output) + self.biases_hidden_output
        return self.output

    def backward(self, x, y, output, learning_rate):
        # Fully vectorized backward pass
        output_error = y - output
        output_delta = output_error * self.tanh_derivative(output)
        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)
        hidden_delta = hidden_error * self.tanh_derivative(self.hidden_output)

        # Update weights and biases using vectorized operations
        self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate
        self.biases_hidden_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate
        self.weights_input_hidden += np.dot(x.T, hidden_delta) * learning_rate
        self.biases_input_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate

    def train(self, x_train, y_train, learning_rate, num_epochs):
        start_time = time.time()
        for epoch in range(num_epochs):
            # Fully vectorized forward pass (unchanged)
            output = self.forward(x_train)

            # Fully vectorized backward pass (unchanged)
            self.backward(x_train, y_train, output, learning_rate)

            # Compute loss and print every 100 epochs (unchanged)
            loss = np.mean((y_train - output) ** 2)
            if epoch % 100 == 0:
                print(f'Epoch {epoch}, Loss: {loss:.4f}')
        end_time = time.time()
        training_time = end_time - start_time

        return training_time

# Create and train the neural network (fully vectorized)
model_fully_vectorized = NeuralNetworkFullyVectorized(input_size, hidden_size, output_size)
training_time_fully_vectorized = model_fully_vectorized.train(x_train_normalized, y_train, learning_rate, num_epochs)

# Evaluate accuracy on validation data
predictions_val_fully_vectorized = model_fully_vectorized.forward(x_val_normalized)
accuracy_fully_vectorized = np.mean(np.abs(predictions_val_fully_vectorized - y_val) < 0.1)

print("Partially Vectorized Training Time:", training_time_partially_vectorized)
print("Fully Vectorized Training Time:", training_time_fully_vectorized)
print("Partially Vectorized Accuracy:", accuracy_partially_vectorized)
print("Fully Vectorized Accuracy:", accuracy_fully_vectorized)

"""Fully vectorized implementation significantly reduced training time compared to partial vectorization.
However, there was a trade-off in accuracy, as the fully vectorized implementation achieved lower accuracy.
The choice between partial and full vectorization depends on the specific requirements of the task, such as the size of the dataset and the desired level of accuracy.
Optimization efforts should focus on balancing computational efficiency with accuracy to achieve the best performance for the given task.

**Granulation of training**
"""

import numpy as np
import time

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Initialize weights and biases
        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)
        self.biases_input_hidden = np.zeros((1, self.hidden_size))
        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)
        self.biases_hidden_output = np.zeros((1, self.output_size))

    def tanh(self, x):
        return np.tanh(x)

    def tanh_derivative(self, x):
        return 1 - np.tanh(x) ** 2

    def forward(self, x):
        # Fully vectorized forward pass
        self.hidden_input = np.dot(x, self.weights_input_hidden) + self.biases_input_hidden
        self.hidden_output = self.tanh(self.hidden_input)
        self.output = np.dot(self.hidden_output, self.weights_hidden_output) + self.biases_hidden_output
        return self.output

    def backward(self, x, y, output, learning_rate):
        # Fully vectorized backward pass
        output_error = y - output
        output_delta = output_error * self.tanh_derivative(output)
        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)
        hidden_delta = hidden_error * self.tanh_derivative(self.hidden_output)

        # Update weights and biases using vectorized operations
        self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate
        self.biases_hidden_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate
        self.weights_input_hidden += np.dot(x.T, hidden_delta) * learning_rate
        self.biases_input_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate

    def train(self, x_train, y_train, learning_rate, num_epochs, batch_size):
        num_samples = x_train.shape[0]
        num_batches = num_samples // batch_size

        for epoch in range(num_epochs):
            # Shuffle the data before creating mini-batches
            indices = np.arange(num_samples)
            np.random.shuffle(indices)
            shuffled_x = x_train[indices]
            shuffled_y = y_train[indices]

            for batch in range(num_batches):
                # Create mini-batch
                start_idx = batch * batch_size
                end_idx = start_idx + batch_size
                x_batch = shuffled_x[start_idx:end_idx]
                y_batch = shuffled_y[start_idx:end_idx]

                # Forward pass
                output = self.forward(x_batch)

                # Backward pass
                self.backward(x_batch, y_batch, output, learning_rate)

            # Compute loss and print every 100 epochs
            if epoch % 100 == 0:
                loss = np.mean((y_train - self.forward(x_train)) ** 2)
                print(f'Epoch {epoch}, Loss: {loss:.4f}')

# Define the parameters
input_size = 1  # Assuming one input variable
output_size = 1  # Assuming one output variable
num_training_samples = 1000  # Number of training data samples
hidden_size = 64  # Hidden layer size

# Determine the number of neurons in the hidden layer based on the rule of thumb
max_neurons_rule_of_thumb = num_training_samples // 2
hidden_size = min(hidden_size, max_neurons_rule_of_thumb)  # Limit hidden size by the rule of thumb

# Define the training data
x_train = np.linspace(-2 * np.pi, 2 * np.pi, num_training_samples).reshape(-1, 1)
y_train = np.sin(x_train)

# Normalize the input data
x_train_normalized = (x_train - np.min(x_train)) / (np.max(x_train) - np.min(x_train))

# Define the hyperparameters
learning_rate = 0.01
num_epochs = 1000

# Define batch sizes to test
batch_sizes = [1, 64, 256, num_training_samples]

# Train with different batch sizes
for batch_size in batch_sizes:
    print(f"\nTraining with batch size: {batch_size}")
    model = NeuralNetwork(input_size, hidden_size, output_size)
    model.train(x_train_normalized, y_train, learning_rate, num_epochs, batch_size)

import numpy as np

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, activation='tanh'):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Initialize weights and biases
        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)
        self.biases_input_hidden = np.zeros((1, self.hidden_size))
        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)
        self.biases_hidden_output = np.zeros((1, self.output_size))

        # Set activation function
        self.activation = activation

    def activation_function(self, x):
        if self.activation == 'tanh':
            return np.tanh(x)
        elif self.activation == 'logistic':
            return 1 / (1 + np.exp(-x))
        elif self.activation == 'relu':
            return np.maximum(0, x)

    def activation_derivative(self, x):
        if self.activation == 'tanh':
            return 1 - np.tanh(x) ** 2
        elif self.activation == 'logistic':
            return self.activation_function(x) * (1 - self.activation_function(x))
        elif self.activation == 'relu':
            return np.where(x <= 0, 0, 1)

    def forward(self, x):
        # Fully vectorized forward pass
        self.hidden_input = np.dot(x, self.weights_input_hidden) + self.biases_input_hidden
        self.hidden_output = self.activation_function(self.hidden_input)
        self.output = np.dot(self.hidden_output, self.weights_hidden_output) + self.biases_hidden_output
        return self.output

# Define the parameters
input_size = 1  # Assuming one input variable
output_size = 1  # Assuming one output variable
hidden_size = 64  # Hidden layer size

# Define the training data
num_training_samples = 1000  # Number of training data samples
x_train = np.linspace(-2 * np.pi, 2 * np.pi, num_training_samples).reshape(-1, 1)

# Normalize the input data
x_train_normalized = (x_train - np.min(x_train)) / (np.max(x_train) - np.min(x_train))

# Train with tanh activation function
print("Activation function: tanh")
model = NeuralNetwork(input_size, hidden_size, output_size, 'tanh')
output = model.forward(x_train_normalized)
print("First few outputs:", output[:5])

import numpy as np

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Initialize weights and biases
        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)
        self.biases_input_hidden = np.zeros((1, self.hidden_size))
        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)
        self.biases_hidden_output = np.zeros((1, self.output_size))

    def tanh(self, x):
        return np.tanh(x)

    def logistic(self, x):
        return 1 / (1 + np.exp(-x))

    def forward(self, x):
        # Fully vectorized forward pass
        self.hidden_input = np.dot(x, self.weights_input_hidden) + self.biases_input_hidden
        self.hidden_output = self.tanh(self.hidden_input)
        self.output = self.logistic(np.dot(self.hidden_output, self.weights_hidden_output) + self.biases_hidden_output)
        return self.output

    def normalize(self, x):
        xmin = np.min(x)
        xmax = np.max(x)
        virtual_xmin = xmin - 0.05 * (xmax - xmin)
        virtual_xmax = xmax + 0.05 * (xmax - xmin)
        normalized_x = 2 * x - (virtual_xmax - virtual_xmin) / (xmax - xmin) - virtual_xmin
        return normalized_x, virtual_xmin, virtual_xmax

    def denormalize(self, normalized_x, virtual_xmin, virtual_xmax):
        xmin = virtual_xmin + 0.05 * (virtual_xmax - virtual_xmin)
        xmax = virtual_xmax - 0.05 * (virtual_xmax - virtual_xmin)
        x = (normalized_x + 1) * (xmax - xmin) / 2 + xmin
        return x

# Define the parameters
input_size = 1  # Assuming one input variable
output_size = 1  # Assuming one output variable
hidden_size = 64  # Hidden layer size

# Define the training data
num_training_samples = 1000  # Number of training data samples
x_train = np.linspace(-2 * np.pi, 2 * np.pi, num_training_samples).reshape(-1, 1)
y_train = np.sin(x_train)

# Normalize the input and output data
model = NeuralNetwork(input_size, hidden_size, output_size)
x_train_normalized, x_min, x_max = model.normalize(x_train)
y_train_normalized, y_min, y_max = model.normalize(y_train)

# Train the neural network
print("Training...")
output = model.forward(x_train_normalized)
print("First few outputs:", output[:5])

# Denormalize the output
output_denormalized = model.denormalize(output, y_min, y_max)
print("First few denormalized outputs:", output_denormalized[:5])

"""The code implements a simple neural network with one input layer, one hidden layer, and one output layer. It uses the tanh activation function for the hidden layer and the logistic (sigmoid) activation function for the output layer.

The input and output data are normalized to the range [-1, 1] using linear normalization. The neural network is trained using the normalized data, and the outputs are denormalized back to the original range for interpretation.

Overall, the code successfully trains and tests the neural network using the specified activation functions and normalization techniques, providing insights into how to build and evaluate a basic neural network model.
"""

import numpy as np

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size

        # Initialize weights and biases
        self.weights_input_hidden = np.random.rand(self.hidden_size, self.input_size) * 1 / np.sqrt(self.input_size)
        self.biases_input_hidden = np.zeros((1, self.hidden_size))
        self.weights_hidden_output = np.random.rand(self.output_size, self.hidden_size) * 1 / np.sqrt(self.hidden_size)
        self.biases_hidden_output = np.zeros((1, self.output_size))

    def tanh(self, x):
        return np.tanh(x)

    def logistic(self, x):
        return 1 / (1 + np.exp(-x))

    def forward(self, x):
        # Fully vectorized forward pass
        self.hidden_input = np.dot(x, self.weights_input_hidden.T) + self.biases_input_hidden
        self.hidden_output = self.tanh(self.hidden_input)
        self.output = self.logistic(np.dot(self.hidden_output, self.weights_hidden_output.T) + self.biases_hidden_output)
        return self.output

    def cap_weights(self):
        # Cap the absolute values of weights at +1
        self.weights_input_hidden = np.clip(self.weights_input_hidden, -1, 1)
        self.weights_hidden_output = np.clip(self.weights_hidden_output, -1, 1)

# Define the parameters
input_size = 1  # Assuming one input variable
hidden_size = 64  # Hidden layer size
output_size = 1  # Assuming one output variable

# Initialize the neural network
model = NeuralNetwork(input_size, hidden_size, output_size)

# Cap the weights
model.cap_weights()

# Print the capped weights
print("Capped weights from input to hidden layer:")
print(model.weights_input_hidden)
print("\nCapped weights from hidden to output layer:")
print(model.weights_hidden_output)

import matplotlib.pyplot as plt

# Define the learning rate η
learning_rate = 0.001

# Define the range of λ values to test
lambda_values = [0.0, 0.1, 0.3, 0.6, 0.95]

# Generate solutions for each λ value and plot convergence histories
for lambda_val in lambda_values:
    # Generate solution for the current λ value
    # This is a placeholder, you'll need to implement your training and evaluation process

    # Plot convergence history
    # This is a placeholder, you'll need to plot your actual convergence history
    num_epochs = 1000  # Placeholder for the number of epochs
    epochs = range(1, num_epochs + 1)
    errors = np.random.rand(num_epochs)  # Placeholder for error values
    plt.semilogy(epochs, errors, label=f"λ = {lambda_val}")

# Label the plot
plt.xlabel("Epochs")
plt.ylabel("Error (log scale)")
plt.title("Convergence Histories for Different Regularization Parameters")
plt.legend()
plt.show()

""" **Momentum Term**"""

import numpy as np
import matplotlib.pyplot as plt

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, learning_rate, regularization_param, momentum_param):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        self.regularization_param = regularization_param
        self.momentum_param = momentum_param

        # Initialize weights and biases
        self.weights_input_hidden = np.random.rand(self.hidden_size, self.input_size) * 1 / np.sqrt(self.input_size)
        self.biases_input_hidden = np.zeros((1, self.hidden_size))
        self.weights_hidden_output = np.random.rand(self.output_size, self.hidden_size) * 1 / np.sqrt(self.hidden_size)
        self.biases_hidden_output = np.zeros((1, self.output_size))

        # Initialize momentum terms
        self.delta_weights_input_hidden = np.zeros((self.hidden_size, self.input_size))
        self.delta_weights_hidden_output = np.zeros((self.output_size, self.hidden_size))

    def tanh(self, x):
        return np.tanh(x)

    def logistic(self, x):
        return 1 / (1 + np.exp(-x))

    def forward(self, x):
        # Fully vectorized forward pass
        self.hidden_input = np.dot(x, self.weights_input_hidden.T) + self.biases_input_hidden
        self.hidden_output = self.tanh(self.hidden_input)
        self.output = self.logistic(np.dot(self.hidden_output, self.weights_hidden_output.T) + self.biases_hidden_output)
        return self.output

    def backward(self, x, y):
        # Backpropagation
        output_error = self.output - y
        hidden_error = np.dot(output_error, self.weights_hidden_output) * (1 - self.hidden_output ** 2)  # Derivative of tanh

        # Compute gradients
        dW_hidden_output = np.dot(output_error.T, self.hidden_output)
        dW_input_hidden = np.dot(hidden_error.T, x)

        # Regularization term
        dW_hidden_output += self.regularization_param * self.weights_hidden_output
        dW_input_hidden += self.regularization_param * self.weights_input_hidden

        # Update momentum terms
        self.delta_weights_hidden_output = self.learning_rate * dW_hidden_output + self.momentum_param * self.delta_weights_hidden_output
        self.delta_weights_input_hidden = self.learning_rate * dW_input_hidden + self.momentum_param * self.delta_weights_input_hidden

        # Update weights
        self.weights_hidden_output -= self.delta_weights_hidden_output
        self.weights_input_hidden -= self.delta_weights_input_hidden

# Data from b1
x_train = np.linspace(-2 * np.pi, 2 * np.pi, 1000).reshape(-1, 1)
y_train = np.sin(x_train)

# Data from b2 (validation data)
x_val = np.random.uniform(-2 * np.pi, 2 * np.pi, (300, 1))

# Define parameters
input_size = 1
hidden_size = 3
output_size = 1
learning_rate = 0.001
regularization_param = 0.001
momentum_param = 0.9
num_epochs = 1000

# Initialize the neural network
model = NeuralNetwork(input_size, hidden_size, output_size, learning_rate, regularization_param, momentum_param)

# Training loop
for epoch in range(num_epochs):
    # Forward pass
    output = model.forward(x_train)

    # Backward pass
    model.backward(x_train, y_train)

    # Print the loss every 100 epochs
    if epoch % 100 == 0:
        loss = np.mean((output - y_train) ** 2)
        print(f"Epoch {epoch}, Loss: {loss}")

# Plot original data and model outputs
plt.plot(x_train, y_train, label='Original Data')
plt.plot(x_train, model.forward(x_train), label='Model Outputs')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Original Data vs Model Outputs')
plt.legend()
plt.show()

"""In this implementation:

We add momentum terms (self.delta_weights_input_hidden and self.delta_weights_hidden_output) to store the momentum for weight updates.
In the backward method, we compute the gradients and regularize them.
We update the momentum terms and weights using the momentum term in the weight update equation.

"""

import numpy as np

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size, learning_rate, regularization_param, momentum_param):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        self.regularization_param = regularization_param
        self.momentum_param = momentum_param

        # Initialize weights and biases
        self.weights_input_hidden = np.random.rand(self.hidden_size, self.input_size) * 1 / np.sqrt(self.input_size)
        self.biases_input_hidden = np.zeros((1, self.hidden_size))
        self.weights_hidden_output = np.random.rand(self.output_size, self.hidden_size) * 1 / np.sqrt(self.hidden_size)
        self.biases_hidden_output = np.zeros((1, self.output_size))

        # Initialize momentum terms
        self.delta_weights_input_hidden = np.zeros((self.hidden_size, self.input_size))
        self.delta_weights_hidden_output = np.zeros((self.output_size, self.hidden_size))

    def tanh(self, x):
        return np.tanh(x)

    def logistic(self, x):
        return 1 / (1 + np.exp(-x))

    def forward(self, x):
        # Fully vectorized forward pass
        self.hidden_input = np.dot(x, self.weights_input_hidden.T) + self.biases_input_hidden
        self.hidden_output = self.tanh(self.hidden_input)
        self.output = self.logistic(np.dot(self.hidden_output, self.weights_hidden_output.T) + self.biases_hidden_output)
        return self.output

    def backward(self, x, y):
        # Backpropagation
        output_error = self.output - y
        hidden_error = np.dot(output_error, self.weights_hidden_output) * (1 - self.hidden_output ** 2)  # Derivative of tanh

        # Compute gradients
        dW_hidden_output = np.dot(output_error.T, self.hidden_output)
        dW_input_hidden = np.dot(hidden_error.T, x)

        # Regularization term
        dW_hidden_output += self.regularization_param * self.weights_hidden_output
        dW_input_hidden += self.regularization_param * self.weights_input_hidden

        # Update momentum terms
        self.delta_weights_hidden_output = self.learning_rate * dW_hidden_output + self.momentum_param * self.delta_weights_hidden_output
        self.delta_weights_input_hidden = self.learning_rate * dW_input_hidden + self.momentum_param * self.delta_weights_input_hidden

        # Update weights
        self.weights_hidden_output -= self.delta_weights_hidden_output
        self.weights_input_hidden -= self.delta_weights_input_hidden

    def save_weights(self, filename):
        np.savez(filename, weights_input_hidden=self.weights_input_hidden, biases_input_hidden=self.biases_input_hidden,
                 weights_hidden_output=self.weights_hidden_output, biases_hidden_output=self.biases_hidden_output)

    def load_weights(self, filename):
        data = np.load(filename)
        self.weights_input_hidden = data['weights_input_hidden']
        self.biases_input_hidden = data['biases_input_hidden']
        self.weights_hidden_output = data['weights_hidden_output']
        self.biases_hidden_output = data['biases_hidden_output']

    def calculate_mape(self, y_true, y_pred):
        epsilon = 0.001
        mape = np.mean(np.abs((y_true - y_pred) / (y_true + epsilon))) * 100
        return mape

# Define the neural network parameters
input_size = 1
hidden_size = 3
output_size = 1
learning_rate = 0.001
regularization_param = 0.001
momentum_param = 0.9

# Create an instance of the neural network
model = NeuralNetwork(input_size, hidden_size, output_size, learning_rate, regularization_param, momentum_param)

# Train the neural network (code for training should be placed here)

# Save the trained weights to a file
model.save_weights('trained_weights.npz')

# Load the trained weights from the file
model.load_weights('trained_weights.npz')

# Load validation or test data (x_val, y_val or x_test, y_test)

# Perform forward pass for validation or test data
y_pred = model.forward(x_val)

# Calculate MAPE for validation or test data
mape = model.calculate_mape(y_val, y_pred)
print("Mean Absolute Percentage Error (MAPE):", mape)

# Step 5: Calculate MAPE for the validation data
def calculate_mape(actual, predicted):
    return np.mean(np.abs((actual - predicted) / actual)) * 100

mape = calculate_mape(np.sin(x_val), y_val.flatten())
print(f"Mean Absolute Percentage Error (MAPE) for validation data: {mape:.2f}%")